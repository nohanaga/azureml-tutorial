{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise04 : Train on Remote GPU Virtual Machine\n",
    "\n",
    "Now we run our previous sample (see \"[Exercise03 : Just Train in Your Working Machine](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/blob/master/notebooks/exercise03_train_simple.ipynb)\") on remote virtual machine with GPU utilized.    \n",
    "Here we use remote virtual machine and conda virtual environment, but you can also use Batch AI pool sharing in your team, or run on your favorite docker images.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your training script as file (train.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ```scirpt``` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please add the following ```%%writefile``` at the beginning of the source code in \"[Exercise03 : Just Train in Your Working Machine](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/blob/master/notebooks/exercise03_train_simple.ipynb)\", and run this cell.    \n",
    "Then this source code is saved as ```./script/train.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/train.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "batch_size = 100\n",
    "\n",
    "#\n",
    "# define functions for Estimator\n",
    "#\n",
    "\n",
    "def _my_input_fn(filepath, num_epochs):\n",
    "    # image - 784 (=28 x 28) elements of grey-scaled integer value [0, 1]\n",
    "    # label - digit (0, 1, ..., 9)\n",
    "    data_queue = tf.train.string_input_producer(\n",
    "        [filepath],\n",
    "        num_epochs = num_epochs) # data is repeated and it raises OutOfRange when data is over\n",
    "    data_reader = tf.TFRecordReader()\n",
    "    _, serialized_exam = data_reader.read(data_queue)\n",
    "    data_exam = tf.parse_single_example(\n",
    "        serialized_exam,\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    data_image = tf.decode_raw(data_exam['image_raw'], tf.uint8)\n",
    "    data_image.set_shape([784])\n",
    "    data_image = tf.cast(data_image, tf.float32) * (1. / 255)\n",
    "    data_label = tf.cast(data_exam['label'], tf.int32)\n",
    "    data_batch_image, data_batch_label = tf.train.batch(\n",
    "        [data_image, data_label],\n",
    "        batch_size=batch_size)\n",
    "    return {'inputs': data_batch_image}, data_batch_label\n",
    "\n",
    "def _get_input_fn(filepath, num_epochs):\n",
    "    return lambda: _my_input_fn(filepath, num_epochs)\n",
    "\n",
    "def _my_model_fn(features, labels, mode):\n",
    "    # with tf.device(...): # You can set device if using GPUs\n",
    "\n",
    "    # define network and inference\n",
    "    # (simple 2 fully connected hidden layer : 784->128->64->10)\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [784, FLAGS.first_layer],\n",
    "                stddev=1.0 / math.sqrt(float(784))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.first_layer]),\n",
    "            name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(features['inputs'], weights) + biases)\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.first_layer, FLAGS.second_layer],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.first_layer))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.second_layer]),\n",
    "            name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.second_layer, 10],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.second_layer))),\n",
    "        name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([10]),\n",
    "            name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    " \n",
    "    # compute evaluation matrix\n",
    "    predicted_indices = tf.argmax(input=logits, axis=1)\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        label_indices = tf.cast(labels, tf.int32)\n",
    "        accuracy = tf.metrics.accuracy(label_indices, predicted_indices)\n",
    "        tf.summary.scalar('accuracy', accuracy[1]) # output to TensorBoard\n",
    " \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=labels,\n",
    "            logits=logits)\n",
    " \n",
    "    # define operations\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        #global_step = tf.train.create_global_step()\n",
    "        #global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        global_step = tf.train.get_or_create_global_step()        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=FLAGS.learning_rate)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        eval_metric_ops = {\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
    "        predictions = {\n",
    "            'classes': predicted_indices,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "        export_outputs = {\n",
    "            'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            predictions=predictions,\n",
    "            export_outputs=export_outputs)\n",
    "\n",
    "def _my_serving_input_fn():\n",
    "    inputs = {'inputs': tf.placeholder(tf.float32, [None, 784])}\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
    "\n",
    "#\n",
    "# Main\n",
    "#\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--data_folder',\n",
    "    type=str,\n",
    "    default='./data',\n",
    "    help='Folder path for input data')\n",
    "parser.add_argument(\n",
    "    '--chkpoint_folder',\n",
    "    type=str,\n",
    "    default='./logs',  # AML experiments logs folder\n",
    "    help='Folder path for checkpoint files')\n",
    "parser.add_argument(\n",
    "    '--model_folder',\n",
    "    type=str,\n",
    "    default='./outputs',  # AML experiments outputs folder\n",
    "    help='Folder path for model output')\n",
    "parser.add_argument(\n",
    "    '--learning_rate',\n",
    "    type=float,\n",
    "    default='0.07',\n",
    "    help='Learning Rate')\n",
    "parser.add_argument(\n",
    "    '--first_layer',\n",
    "    type=int,\n",
    "    default='128',\n",
    "    help='Neuron number for the first hidden layer')\n",
    "parser.add_argument(\n",
    "    '--second_layer',\n",
    "    type=int,\n",
    "    default='64',\n",
    "    help='Neuron number for the second hidden layer')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# clean checkpoint and model folder if exists\n",
    "if os.path.exists(FLAGS.chkpoint_folder) :\n",
    "    for file_name in os.listdir(FLAGS.chkpoint_folder):\n",
    "        file_path = os.path.join(FLAGS.chkpoint_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "if os.path.exists(FLAGS.model_folder) :\n",
    "    for file_name in os.listdir(FLAGS.model_folder):\n",
    "        file_path = os.path.join(FLAGS.model_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "\n",
    "# read TF_CONFIG\n",
    "run_config = tf.estimator.RunConfig()\n",
    "\n",
    "# create Estimator\n",
    "mnist_fullyconnected_classifier = tf.estimator.Estimator(\n",
    "    model_fn=_my_model_fn,\n",
    "    model_dir=FLAGS.chkpoint_folder,\n",
    "    config=run_config)\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'train.tfrecords'), 2),\n",
    "    max_steps=60000 * 2 / batch_size)\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'test.tfrecords'), 1),\n",
    "    steps=10000 * 1 / batch_size,\n",
    "    start_delay_secs=0)\n",
    "\n",
    "# run !\n",
    "tf.estimator.train_and_evaluate(\n",
    "    mnist_fullyconnected_classifier,\n",
    "    train_spec,\n",
    "    eval_spec\n",
    ")\n",
    "\n",
    "# save model and variables\n",
    "model_dir = mnist_fullyconnected_classifier.export_savedmodel(\n",
    "    export_dir_base = FLAGS.model_folder,\n",
    "    serving_input_receiver_fn = _my_serving_input_fn)\n",
    "print('current working directory is ', os.getcwd())\n",
    "print('model is saved ', model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on remote VM\n",
    "\n",
    "Now let's start to integrate with AML services and run training on remote virtual machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Get workspace setting\n",
    "\n",
    "Before starting, you must read your configuration settings. (See \"[Exercise01 : Prepare Config Settings](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/blob/master/notebooks/exercise01_prepare_config.ipynb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import azureml.core\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Create new remote virtual machine\n",
    "\n",
    "Create your new Data Science Virtual Machine (which is pre-configured for data science) with **GPU** (NC6). Before starting, please make sure to use NC6 supported location as workspace location. By enabling auto-scaling (from 0 to 1), you can save money (the node is terminated) if it's inactive.    \n",
    "If already exists, this script will get the existing one.\n",
    "\n",
    "You can also attach an existing virtual machine (bring your own compute resource) as a compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new.\n",
      "InProgress......\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name='mydsvm01')\n",
    "    print('found existing:', compute_target.name)\n",
    "except ComputeTargetException:\n",
    "    print('creating new.')\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='STANDARD_NC6',\n",
    "        min_nodes=0,\n",
    "        max_nodes=1)\n",
    "    compute_target = ComputeTarget.create(ws, 'mydsvm01', compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Create dataset reference for files\n",
    "\n",
    "You can configure to mount your preconfigured dataset (train.tfrecords, test.tfrecords) in your compute target.    \n",
    "See \"[Exercise02 : Prepare Datastore](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/blob/master/notebooks/exercise02_prepare_datastore.ipynb)\" for data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.core import Dataset\n",
    "\n",
    "# get common datastore (See \"Exercise 02 : Prepare Datastore\")\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# create dataset for files\n",
    "ds_paths = [(ds, 'tfdata/')]\n",
    "dataset = Dataset.File.from_files(path = ds_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Create environment\n",
    "\n",
    "Here we set docker environments for running scripts. In the first time, it will generate our own conatiner image as following settings. (It will then take a long time for completing experiment.)\n",
    "However, you can speed up by reusing the generated environment in the next run, once you have registered the generated environment.\n",
    "\n",
    "In this example, we create our own environment manually, but **you can also use existing environments (called, curated environments) for a variety of purposes**. (It also includes an environment for running TensorFlow 1.x.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn8-ubuntu18.04:20210615.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"test-remote-gpu-env\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.6\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults~=1.32.0\"\n",
       "                    ]\n",
       "                },\n",
       "                \"tensorflow-gpu==1.15\"\n",
       "            ],\n",
       "            \"name\": \"azureml_6a8440b70554a3c0909a05b5a0c036aa\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"1\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "# create environment\n",
    "env = Environment('test-remote-gpu-env')\n",
    "env.python.conda_dependencies = CondaDependencies.create(\n",
    "    python_version=\"3.6\",\n",
    "    conda_packages=['tensorflow-gpu==1.15'])\n",
    "env.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "\n",
    "# register environment to re-use later\n",
    "env.register(workspace=ws)\n",
    "## # speed up by using the existing environment\n",
    "## env = Environment.get(ws, name='test-remote-gpu-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Run script and wait for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: tf_remote_experiment_1626055156_39dfd5b9\n",
      "Web View: https://ml.azure.com/runs/tf_remote_experiment_1626055156_39dfd5b9?wsid=/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourcegroups/TEST20210712/workspaces/ws01&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2021-07-12T01:59:27Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=305476 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-07-12T01:59:27Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/mounts/workspaceblobstore\n",
      "2021-07-12T01:59:27Z Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ". Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      "2021-07-12T01:59:27Z Starting output-watcher...\n",
      "2021-07-12T01:59:27Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-07-12T01:59:27Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2021-07-12T01:59:27Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_fe4afc798de401edfb76dc27a38b1703\n",
      "Digest: sha256:5224cd9c4e07c9304c90193ab084da3cf8643e81065eef4d81c2c4029c58248c\n",
      "Status: Image is up to date for viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
      "viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
      "2021-07-12T01:59:28Z Check if container tf_remote_experiment_1626055156_39dfd5b9_DataSidecar already exist exited with 0, \n",
      "\n",
      "72a9f47d4b8176bbb936cf196a78fc16e70dcfa324b46d2084e8bcfb9549f178\n",
      "2021-07-12T01:59:28Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-07-12T01:59:28Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-b23a79e1b5373e54a0038376ba3cc5c5-5cdea10c962835ef-01 -sshRequired=false] \n",
      "2021/07/12 01:59:28 Starting App Insight Logger for task:  containerSetup\n",
      "2021/07/12 01:59:28 Version: 3.0.01632.0003 Branch: .SourceBranch Commit: 4b96fb0\n",
      "2021/07/12 01:59:28 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/07/12 01:59:28 Starting infiniband setup\n",
      "2021/07/12 01:59:28 Python Version found is Python 3.7.9\n",
      "\n",
      "2021/07/12 01:59:28 Returning Python Version as 3.7\n",
      "2021-07-12T01:59:28Z VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/07/12 01:59:28 VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/07/12 01:59:28 VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/07/12 01:59:28 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021-07-12T01:59:28Z Not setting up Infiniband in Container\n",
      "2021/07/12 01:59:28 Not setting up Infiniband in Container\n",
      "2021/07/12 01:59:28 Not setting up Infiniband in Container\n",
      "2021/07/12 01:59:28 Python Version found is Python 3.7.9\n",
      "\n",
      "2021/07/12 01:59:28 Returning Python Version as 3.7\n",
      "2021/07/12 01:59:28 sshd inside container not required for job, skipping setup.\n",
      "2021/07/12 01:59:29 All App Insights Logs was sent successfully or the close timeout of 20 was reached\n",
      "2021/07/12 01:59:29 App Insight Client has already been closed\n",
      "2021/07/12 01:59:29 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-07-12T01:59:29Z Starting docker container succeeded.\n",
      "2021-07-12T01:59:43Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2021-07-12T01:59:43Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_5c8296d5291e6d339f1d39cd3c55e613\n",
      "Digest: sha256:14803410c3f91af40c51a6a85edc3eb1c5dd32069aa45208a558c4b234ed9b53\n",
      "Status: Image is up to date for b01299e2219e4f20bdda1ac50c15da50.azurecr.io/azureml/azureml_5c8296d5291e6d339f1d39cd3c55e613:latest\n",
      "b01299e2219e4f20bdda1ac50c15da50.azurecr.io/azureml/azureml_5c8296d5291e6d339f1d39cd3c55e613:latest\n",
      "2021-07-12T01:59:43Z Check if container tf_remote_experiment_1626055156_39dfd5b9 already exist exited with 0, 72a9f47d4b81\n",
      "\n",
      "\n",
      "2021-07-12T01:59:43Z The container tf_remote_experiment_1626055156_39dfd5b9 already exists, stop and remove it before starting it.\n",
      "2021-07-12T01:59:43Z Stopping container tf_remote_experiment_1626055156_39dfd5b9 exited with 1, Error response from daemon: No such container: tf_remote_experiment_1626055156_39dfd5b9\n",
      "\n",
      "\n",
      "2021-07-12T01:59:43Z Removing container tf_remote_experiment_1626055156_39dfd5b9 exited with 1, Error: No such container: tf_remote_experiment_1626055156_39dfd5b9\n",
      "\n",
      "\n",
      "80c3defa038a3ede9052b2171e08cee927d6f512ca3b5784966b1e1826387e90\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "2021/07/12 01:59:45 Starting App Insight Logger for task:  runTaskLet\n",
      "2021/07/12 01:59:45 Version: 3.0.01632.0003 Branch: .SourceBranch Commit: 4b96fb0\n",
      "2021/07/12 01:59:45 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
      "2021/07/12 01:59:45 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "[2021-07-12T01:59:45.739680] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['train.py', '--data_folder', 'DatasetConsumptionConfig:input__e12a5186'])\n",
      "Script type = None\n",
      "[2021-07-12T01:59:46.227507] Entering Run History Context Manager.\n",
      "[2021-07-12T01:59:46.851523] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/wd/azureml/tf_remote_experiment_1626055156_39dfd5b9\n",
      "[2021-07-12T01:59:46.851753] Preparing to call script [train.py] with arguments:['--data_folder', '$input__e12a5186']\n",
      "[2021-07-12T01:59:46.851776] After variable expansion, calling script [train.py] with arguments:['--data_folder', '/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/wd/input__e12a5186_f3adcd26-eecb-4121-8cb8-b4ca49728bee']\n",
      "\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From train.py:25: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/training/input.py:112: RefVariable.count_up_to (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:2522: count_up_to (from tensorflow.python.ops.state_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From train.py:26: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From train.py:28: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:31: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:40: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From train.py:53: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:86: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:87: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:89: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From train.py:97: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:98: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "2021-07-12 01:59:48.577933: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2021-07-12 01:59:48.584000: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596990000 Hz\n",
      "2021-07-12 01:59:48.584803: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5634d89378c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-07-12 01:59:48.584833: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-07-12 01:59:48.585886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-12 01:59:48.626836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: efcd:00:00.0\n",
      "2021-07-12 01:59:48.627096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-07-12 01:59:48.628435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-07-12 01:59:48.629681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-07-12 01:59:48.629974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-07-12 01:59:48.631575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-07-12 01:59:48.632794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-07-12 01:59:48.636584: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-07-12 01:59:48.638027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2021-07-12 01:59:48.638074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-07-12 01:59:48.786979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-12 01:59:48.787029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2021-07-12 01:59:48.787038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2021-07-12 01:59:48.789246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10812 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: efcd:00:00.0, compute capability: 3.7)\n",
      "2021-07-12 01:59:48.791864: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5634d9a570d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-07-12 01:59:48.791883: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py:882: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "2021-07-12 01:59:49.558812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021/07/12 01:59:50 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-12 02:00:00.235157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: efcd:00:00.0\n",
      "2021-07-12 02:00:00.235227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-07-12 02:00:00.235246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-07-12 02:00:00.235261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-07-12 02:00:00.235276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-07-12 02:00:00.235290: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-07-12 02:00:00.235304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-07-12 02:00:00.235320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-07-12 02:00:00.235970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2021-07-12 02:00:00.236015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-12 02:00:00.236024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2021-07-12 02:00:00.236031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2021-07-12 02:00:00.236804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10812 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: efcd:00:00.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From train.py:212: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function has been renamed, use `export_saved_model` instead.\n",
      "WARNING:tensorflow:From train.py:130: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_6a8440b70554a3c0909a05b5a0c036aa/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "2021-07-12 02:00:01.676010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: efcd:00:00.0\n",
      "2021-07-12 02:00:01.676072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-07-12 02:00:01.676090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-07-12 02:00:01.676106: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-07-12 02:00:01.676120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-07-12 02:00:01.676134: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-07-12 02:00:01.676148: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-07-12 02:00:01.676162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-07-12 02:00:01.676798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2021-07-12 02:00:01.676832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-12 02:00:01.676841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2021-07-12 02:00:01.676847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2021-07-12 02:00:01.677530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10812 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: efcd:00:00.0, compute capability: 3.7)\n",
      "current working directory is  /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/wd/azureml/tf_remote_experiment_1626055156_39dfd5b9\n",
      "model is saved  b'./outputs/1626055201'\n",
      "\n",
      "\n",
      "[2021-07-12T02:00:01.735159] The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
      "1 items cleaning up...\n",
      "Cleanup took 0.1311805248260498 seconds\n",
      "[2021-07-12T02:00:02.026933] Finished context manager injector.\n",
      "2021/07/12 02:00:03 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "2021/07/12 02:00:03 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 2\n",
      "FilteredData: 0.\n",
      "2021/07/12 02:00:03 Process Exiting with Code:  0\n",
      "2021/07/12 02:00:03 All App Insights Logs was sent successfully or the close timeout of 20 was reached\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2021-07-12T02:00:04.692530] Entering job release\n",
      "[2021-07-12T02:00:05.504148] Starting job release\n",
      "[2021-07-12T02:00:05.504538] Logging experiment finalizing status in history service.\n",
      "[2021-07-12T02:00:05.504748] job release stage : upload_datastore starting...Starting the daemon thread to refresh tokens in background for process with pid = 323\n",
      "[2021-07-12T02:00:05.505164] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "\n",
      "[2021-07-12T02:00:05.505242] job release stage : execute_job_release starting...\n",
      "[2021-07-12T02:00:05.505669] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-07-12T02:00:05.510266] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-07-12T02:00:05.517759] Entering context manager injector.\n",
      "[2021-07-12T02:00:05.521830] job release stage : upload_datastore completed...\n",
      "[2021-07-12T02:00:05.606665] job release stage : send_run_telemetry starting...\n",
      "[2021-07-12T02:00:05.618892] job release stage : execute_job_release completed...\n",
      "[2021-07-12T02:00:05.620721] get vm size and vm region successfully.\n",
      "[2021-07-12T02:00:05.627198] get compute meta data successfully.\n",
      "[2021-07-12T02:00:05.883181] post artifact meta request successfully.\n",
      "[2021-07-12T02:00:05.907061] upload compute record artifact successfully.\n",
      "[2021-07-12T02:00:05.907155] job release stage : send_run_telemetry completed...\n",
      "[2021-07-12T02:00:05.907648] Running in AzureML-Sidecar, starting to exit user context managers...\n",
      "[2021-07-12T02:00:05.907742] Running Sidecar release cmd...\n",
      "[2021-07-12T02:00:05.929836] INFO azureml.sidecar.sidecar: Received task: exit_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/wd/azureml/tf_remote_experiment_1626055156_39dfd5b9\n",
      "Enter __exit__ of DatasetContextManager\n",
      "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/wd/input__e12a5186_f3adcd26-eecb-4121-8cb8-b4ca49728bee.\n",
      "fuse: failed to unmount /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/wd/input__e12a5186_f3adcd26-eecb-4121-8cb8-b4ca49728bee: Invalid argument\n",
      "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1626055156_39dfd5b9/wd/input__e12a5186_f3adcd26-eecb-4121-8cb8-b4ca49728bee.\n",
      "Exit __exit__ of DatasetContextManager\n",
      "[2021-07-12T02:00:05.968278] Removing absolute paths from host...\n",
      "[2021-07-12T02:00:05.968478] INFO azureml.sidecar.task.exit_contexts: Exited Context Managers\n",
      "[2021-07-12T02:00:06.536188] Ran Sidecar release cmd.\n",
      "[2021-07-12T02:00:06.536268] Job release is complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: tf_remote_experiment_1626055156_39dfd5b9\n",
      "Web View: https://ml.azure.com/runs/tf_remote_experiment_1626055156_39dfd5b9?wsid=/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourcegroups/TEST20210712/workspaces/ws01&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'tf_remote_experiment_1626055156_39dfd5b9',\n",
       " 'target': 'mydsvm01',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2021-07-12T01:59:25.751106Z',\n",
       " 'endTimeUtc': '2021-07-12T02:00:15.037829Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': '4385651e-6aa6-4bf0-8ec3-182bc2841a1d',\n",
       "  'azureml.git.repository_uri': 'https://github.com/tsmatz/azure-ml-tensorflow-complete-sample.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/tsmatz/azure-ml-tensorflow-complete-sample.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': 'b5e83f1c910ce7bde6489279614a6dace708dfed',\n",
       "  'mlflow.source.git.commit': 'b5e83f1c910ce7bde6489279614a6dace708dfed',\n",
       "  'azureml.git.dirty': 'True',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json',\n",
       "  'azureml.RuntimeType': ''},\n",
       " 'inputDatasets': [{'dataset': {'id': 'f3adcd26-eecb-4121-8cb8-b4ca49728bee'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'input__e12a5186', 'mechanism': 'Mount'}}],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': 'train.py',\n",
       "  'command': '',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data_folder', 'DatasetConsumptionConfig:input__e12a5186'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'mydsvm01',\n",
       "  'dataReferences': {},\n",
       "  'data': {'input__e12a5186': {'dataLocation': {'dataset': {'id': 'f3adcd26-eecb-4121-8cb8-b4ca49728bee',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None,\n",
       "     'uri': None},\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'input__e12a5186',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'datacaches': [],\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': 2592000,\n",
       "  'nodeCount': 1,\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'identity': None,\n",
       "  'environment': {'name': 'test-remote-gpu-env',\n",
       "   'version': 'Autosave_2021-07-12T01:30:14Z_8c462d71',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6',\n",
       "      {'pip': ['azureml-defaults~=1.32.0']},\n",
       "      'tensorflow-gpu==1.15'],\n",
       "     'name': 'azureml_6a8440b70554a3c0909a05b5a0c036aa'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn8-ubuntu18.04:20210615.v1',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': False,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': None},\n",
       "  'aiSuperComputer': {'instanceType': None,\n",
       "   'imageVersion': None,\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None,\n",
       "   'interactive': False,\n",
       "   'scalePolicy': None,\n",
       "   'virtualClusterArmId': None,\n",
       "   'tensorboardLogDirectory': None,\n",
       "   'sshPublicKey': None,\n",
       "   'enableAzmlInt': True,\n",
       "   'priority': None,\n",
       "   'slaTier': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []},\n",
       "  'environmentVariables': {},\n",
       "  'applicationEndpoints': {},\n",
       "  'parameters': []},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/azureml-logs/55_azureml-execution-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt?sv=2019-02-02&sr=b&sig=F%2F8ORxmMynHYn9L6uTOAg89VgrnqDcAoDInYVDRlUrw%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/azureml-logs/65_job_prep-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt?sv=2019-02-02&sr=b&sig=%2F97pc789yMvhhyyVgUB8mMcOga%2B1BMXYzx9eMVG27MY%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=MyfiknH1Y%2BVuaSiMy%2Fe4d9TsIc52KN7xCH2CbbQgR7o%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/azureml-logs/75_job_post-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt?sv=2019-02-02&sr=b&sig=naj9bayex6L%2F9jnPvQ8JfAOrlsd40wTAl7oryYp2t7o%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=PnVOLijZTMUrf%2ByyM2gaWC8k0f27kojqaIE7rSx4rBw%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=XNDBI2ckbZdk6EH6kDipMuqmmhUU4JqpWlBnOm9nAyA%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=ne7NMpf%2F9hAHrZUabhWz6VNbNa9vClq3lvS8ezRFnFk%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=6YXht%2BCR7CUrPPeb8PCkSXhJcSQ7v%2BbddXYsRBIe24Q%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=l5jI5zEWH%2B%2BnjjV9dLJjfW%2BflutPf8s8Otb8cgp239Y%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d/all.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/logs/azureml/sidecar/tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d/all.log?sv=2019-02-02&sr=b&sig=vdha5bl4JsrNDSOjweH0riAb4ayYM3DhKxz8EcHh2aE%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d/task.exit_contexts.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1626055156_39dfd5b9/logs/azureml/sidecar/tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=wzupIuo1Ygv8VRN%2B69dH1b1V%2B18%2FASE%2FKQyIGtdnCsc%3D&st=2021-07-12T01%3A50%3A14Z&se=2021-07-12T10%3A00%3A14Z&sp=r'},\n",
       " 'submittedBy': 'Tsuyoshi Matsuzaki'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "# create script run config\n",
    "src = ScriptRunConfig(\n",
    "    source_directory='./script',\n",
    "    script='train.py',\n",
    "    arguments=['--data_folder', dataset.as_mount()],\n",
    "    compute_target=compute_target,\n",
    "    environment=env,\n",
    "    docker_runtime_config=DockerConfiguration(use_docker=True))\n",
    "\n",
    "# submit and run !\n",
    "exp = Experiment(workspace=ws, name='tf_remote_experiment')\n",
    "run = exp.submit(config=src)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Download results and check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check generated files and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/55_azureml-execution-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d.txt',\n",
       " 'azureml-logs/process_info.json',\n",
       " 'azureml-logs/process_status.json',\n",
       " 'logs/azureml/dataprep/backgroundProcess.log',\n",
       " 'logs/azureml/dataprep/backgroundProcess_Telemetry.log',\n",
       " 'logs/azureml/job_release_azureml.log',\n",
       " 'logs/azureml/sidecar/tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d/all.log',\n",
       " 'logs/azureml/sidecar/tvmps_d508cc13984f5d2fb9cc28743496d89065f6d7c0f4e22999b48c598147ad2174_d/task.exit_contexts.log',\n",
       " 'logs/checkpoint',\n",
       " 'logs/eval/events.out.tfevents.1626055201.87001a5c2f604beaa0df0e0a74330bef000001',\n",
       " 'logs/events.out.tfevents.1626055188.87001a5c2f604beaa0df0e0a74330bef000001',\n",
       " 'logs/graph.pbtxt',\n",
       " 'logs/model.ckpt-0.data-00000-of-00001',\n",
       " 'logs/model.ckpt-0.index',\n",
       " 'logs/model.ckpt-0.meta',\n",
       " 'logs/model.ckpt-1100.data-00000-of-00001',\n",
       " 'logs/model.ckpt-1100.index',\n",
       " 'logs/model.ckpt-1100.meta',\n",
       " 'outputs/1626055201/saved_model.pb',\n",
       " 'outputs/1626055201/variables/variables.data-00000-of-00001',\n",
       " 'outputs/1626055201/variables/variables.index']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model into your local machine.    \n",
    "**Please change ```1626055201``` to meet previous results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(\n",
    "    name='outputs/1626055201/saved_model.pb',\n",
    "    output_file_path='remote_model/saved_model.pb')\n",
    "run.download_file(\n",
    "    name='outputs/1626055201/variables/variables.data-00000-of-00001',\n",
    "    output_file_path='remote_model/variables/variables.data-00000-of-00001')\n",
    "run.download_file(\n",
    "    name='outputs/1626055201/variables/variables.index',\n",
    "    output_file_path='remote_model/variables/variables.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict your test data using downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from ./remote_model/variables/variables\n",
      "Predicted:  [7, 2, 1]\n",
      "Actual   :  [7, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Read data by tensor\n",
    "dataset = tf.data.TFRecordDataset('./data/test.tfrecords')\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "data_org = iterator.get_next()\n",
    "data_exam = tf.parse_single_example(\n",
    "    data_org,\n",
    "    features={\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "        'label': tf.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "data_image = tf.decode_raw(data_exam['image_raw'], tf.uint8)\n",
    "data_image.set_shape([784])\n",
    "data_image = tf.cast(data_image, tf.float32) * (1. / 255)\n",
    "data_label = tf.cast(data_exam['label'], tf.int32)\n",
    "\n",
    "# Run tensor and generate data\n",
    "with tf.Session() as sess:\n",
    "    image_arr = []\n",
    "    label_arr = []\n",
    "    for i in range(3):\n",
    "        image, label = sess.run([data_image, data_label])\n",
    "        image_arr.append(image)\n",
    "        label_arr.append(label)\n",
    "\n",
    "# Predict\n",
    "pred_fn = tf.contrib.predictor.from_saved_model('./remote_model')\n",
    "pred = pred_fn({'inputs': image_arr})\n",
    "\n",
    "print('Predicted: ', pred['classes'].tolist())\n",
    "print('Actual   : ', label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 : Remove AML compute\n",
    "\n",
    "**You don't need to remove your AML compute** for saving money, because the nodes will be automatically terminated, when it's inactive.    \n",
    "But if you want to clean up, please run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete cluster (nbodes) and remove from AML workspace\n",
    "mycompute = AmlCompute(workspace=ws, name='mydsvm01')\n",
    "mycompute.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-07-12T01:50:09.333000+00:00', 'errors': None, 'creationTime': '2021-07-12T00:41:40.142824+00:00', 'modifiedTime': '2021-07-12T00:42:05.880362+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "# get a status for the current cluster.\n",
    "print(mycompute.status.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
