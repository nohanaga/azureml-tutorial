{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise05 : Distributed Training with Curated Environments\n",
    "\n",
    "Here we change our sample (see \"[Exercise03 : Just Train in Your Working Machine](https://github.com/tsmatz/azureml-tutorial-tensorflow-v1/blob/master/notebooks/exercise03_train_simple.ipynb)\") for distributed training using multiple machines.\n",
    "\n",
    "Azure Machine Learning provides built-in pre-configured image, called curated environments. (See [here](https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments) for the list of curated environments.)<br>\n",
    "You can run distributed training by Horovod using curated environment ```AzureML-TensorFlow-1.13-CPU```, and here I configure using this curated environment.\n",
    "\n",
    "In this exercise, we use Horovod framework using built-in pre-configured image, called curated environments. (See [here](https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments) for the list of curated environments.)<br>\n",
    "As you saw in previous [Exercise04](https://github.com/tsmatz/azureml-tutorial-tensorflow-v1/blob/master/notebooks/exercise04_train_remote.ipynb), you can also configure distributed training manually using primitive ```azureml.core.ScriptRunConfig``` without curated environments. (See [here](https://tsmatz.wordpress.com/2019/01/17/azure-machine-learning-service-custom-amlcompute-and-runconfig-for-mxnet-distributed-training/) for the manually configured example using ```azureml.core.ScriptRunConfig```.)\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/azureml-tutorial-tensorflow-v1/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your training script as file (train.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ```scirpt``` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change our original source code ```train.py``` (see \"[Exercise03 : Just Train in Your Working Machine](https://github.com/tsmatz/azureml-tutorial-tensorflow-v1/blob/master/notebooks/exercise03_train_simple.ipynb)\") as follows. The lines commented \"##### modified\" is modified lines.    \n",
    "After that, please add the following ```%%writefile``` at the beginning of the source code and run this cell.    \n",
    "This source code is saved as ```./script/train_horovod.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script/train_horovod.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/train_horovod.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow as hvd ##### modified\n",
    "\n",
    "FLAGS = None\n",
    "batch_size = 100\n",
    "\n",
    "#\n",
    "# define functions for Estimator\n",
    "#\n",
    "\n",
    "def _my_input_fn(filepath, num_epochs):\n",
    "    # image - 784 (=28 x 28) elements of grey-scaled integer value [0, 1]\n",
    "    # label - digit (0, 1, ..., 9)\n",
    "    data_queue = tf.train.string_input_producer(\n",
    "        [filepath],\n",
    "        num_epochs = num_epochs) # data is repeated and it raises OutOfRange when data is over\n",
    "    data_reader = tf.TFRecordReader()\n",
    "    _, serialized_exam = data_reader.read(data_queue)\n",
    "    data_exam = tf.parse_single_example(\n",
    "        serialized_exam,\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    data_image = tf.decode_raw(data_exam['image_raw'], tf.uint8)\n",
    "    data_image.set_shape([784])\n",
    "    data_image = tf.cast(data_image, tf.float32) * (1. / 255)\n",
    "    data_label = tf.cast(data_exam['label'], tf.int32)\n",
    "    data_batch_image, data_batch_label = tf.train.batch(\n",
    "        [data_image, data_label],\n",
    "        batch_size=batch_size)\n",
    "    return {'inputs': data_batch_image}, data_batch_label\n",
    "\n",
    "def _get_input_fn(filepath, num_epochs):\n",
    "    return lambda: _my_input_fn(filepath, num_epochs)\n",
    "\n",
    "def _my_model_fn(features, labels, mode):\n",
    "    # with tf.device(...): # You can set device if using GPUs\n",
    "\n",
    "    # define network and inference\n",
    "    # (simple 2 fully connected hidden layer : 784->128->64->10)\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [784, FLAGS.first_layer],\n",
    "                stddev=1.0 / math.sqrt(float(784))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.first_layer]),\n",
    "            name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(features['inputs'], weights) + biases)\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.first_layer, FLAGS.second_layer],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.first_layer))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.second_layer]),\n",
    "            name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.second_layer, 10],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.second_layer))),\n",
    "        name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([10]),\n",
    "            name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    " \n",
    "    # compute evaluation matrix\n",
    "    predicted_indices = tf.argmax(input=logits, axis=1)\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        label_indices = tf.cast(labels, tf.int32)\n",
    "        accuracy = tf.metrics.accuracy(label_indices, predicted_indices)\n",
    "        tf.summary.scalar('accuracy', accuracy[1]) # output to TensorBoard\n",
    " \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=labels,\n",
    "            logits=logits)\n",
    " \n",
    "    # define operations\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        global_step = tf.train.get_or_create_global_step()        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=FLAGS.learning_rate)\n",
    "        optimizer = hvd.DistributedOptimizer(optimizer) ##### modified\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        eval_metric_ops = {\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
    "        predictions = {\n",
    "            'classes': predicted_indices,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "        export_outputs = {\n",
    "            'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            predictions=predictions,\n",
    "            export_outputs=export_outputs)\n",
    "\n",
    "def _my_serving_input_fn():\n",
    "    inputs = {'inputs': tf.placeholder(tf.float32, [None, 784])}\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
    "\n",
    "#\n",
    "# Main\n",
    "#\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--data_folder',\n",
    "    type=str,\n",
    "    default='./data',\n",
    "    help='Folder path for input data')\n",
    "parser.add_argument(\n",
    "    '--chkpoint_folder',\n",
    "    type=str,\n",
    "    default='./logs',  # AML experiments logs folder\n",
    "    help='Folder path for checkpoint files')\n",
    "parser.add_argument(\n",
    "    '--model_folder',\n",
    "    type=str,\n",
    "    default='./outputs',  # AML experiments outputs folder\n",
    "    help='Folder path for model output')\n",
    "parser.add_argument(\n",
    "    '--learning_rate',\n",
    "    type=float,\n",
    "    default='0.07',\n",
    "    help='Learning Rate')\n",
    "parser.add_argument(\n",
    "    '--first_layer',\n",
    "    type=int,\n",
    "    default='128',\n",
    "    help='Neuron number for the first hidden layer')\n",
    "parser.add_argument(\n",
    "    '--second_layer',\n",
    "    type=int,\n",
    "    default='64',\n",
    "    help='Neuron number for the second hidden layer')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# clean checkpoint and model folder if exists\n",
    "if os.path.exists(FLAGS.chkpoint_folder) :\n",
    "    for file_name in os.listdir(FLAGS.chkpoint_folder):\n",
    "        file_path = os.path.join(FLAGS.chkpoint_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "if os.path.exists(FLAGS.model_folder) :\n",
    "    for file_name in os.listdir(FLAGS.model_folder):\n",
    "        file_path = os.path.join(FLAGS.model_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "\n",
    "hvd.init() ##### modified\n",
    "\n",
    "# read TF_CONFIG\n",
    "run_config = tf.estimator.RunConfig()\n",
    "\n",
    "# create Estimator\n",
    "mnist_fullyconnected_classifier = tf.estimator.Estimator(\n",
    "    model_fn=_my_model_fn,\n",
    "    model_dir=FLAGS.chkpoint_folder if hvd.rank() == 0 else None, ##### modified\n",
    "    config=run_config)\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'train.tfrecords'), 2),\n",
    "    #max_steps=60000 * 2 / batch_size)\n",
    "    max_steps=(60000 * 2 / batch_size) // hvd.size(), ##### modified\n",
    "    hooks=[hvd.BroadcastGlobalVariablesHook(0)]) ##### modified\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'test.tfrecords'), 1),\n",
    "    steps=10000 * 1 / batch_size,\n",
    "    start_delay_secs=0)\n",
    "\n",
    "# run !\n",
    "tf.estimator.train_and_evaluate(\n",
    "    mnist_fullyconnected_classifier,\n",
    "    train_spec,\n",
    "    eval_spec\n",
    ")\n",
    "\n",
    "# save model and variables\n",
    "if hvd.rank() == 0 : ##### modified\n",
    "    model_dir = mnist_fullyconnected_classifier.export_savedmodel(\n",
    "        export_dir_base = FLAGS.model_folder,\n",
    "        serving_input_receiver_fn = _my_serving_input_fn)\n",
    "    print('current working directory is ', os.getcwd())\n",
    "    print('model is saved ', model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on multiple machines (Horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Get workspace setting\n",
    "\n",
    "Before starting, you must read your configuration settings. (See \"[Exercise01 : Prepare Config Settings](https://github.com/tsmatz/azureml-tutorial-tensorflow-v1/blob/master/notebooks/exercise01_prepare_config.ipynb)\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import azureml.core\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Create multiple virtual machines (cluster)\n",
    "\n",
    "Create your new AML compute for distributed clusters. By enabling auto-scaling from 0 to 4, you can save money (all nodes are terminated) if it's inactive.    \n",
    "If already exists, this script will get the existing cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new.\n",
      "InProgress......\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name='mycluster01')\n",
    "    print('found existing:', compute_target.name)\n",
    "except ComputeTargetException:\n",
    "    print('creating new.')\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='Standard_D2_v2',\n",
    "        min_nodes=0,\n",
    "        max_nodes=3)\n",
    "    compute_target = ComputeTarget.create(ws, 'mycluster01', compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-07-12T02:11:06.030000+00:00', 'errors': None, 'creationTime': '2021-07-12T02:10:43.391839+00:00', 'modifiedTime': '2021-07-12T02:11:09.118976+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 3, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
     ]
    }
   ],
   "source": [
    "# get a status for the current cluster.\n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Get dataset reference for files\n",
    "\n",
    "You can mount your registered dataset (See \"[Exercise02 : Prepare Data](https://github.com/tsmatz/azureml-tutorial-tensorflow-v1/blob/master/notebooks/exercise02_prepare_data.ipynb)\") into your AML compute.<br>\n",
    "Now we get the registered dataset reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "dataset = Dataset.get_by_name(ws, 'mnist_tfrecords_dataset', version='latest')\n",
    "\n",
    "# # For using unregistered data, see below\n",
    "# from azureml.core import Datastore\n",
    "# from azureml.core import Dataset\n",
    "# ds = ws.get_default_datastore()\n",
    "# ds_paths = [(ds, 'tfdata/')]\n",
    "# dataset = Dataset.File.from_files(path = ds_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Configure with Curated Environment\n",
    "\n",
    "Unlike the previous exercise (see \"[Exercise04 : Train on Remote GPU Virtual Machine](https://github.com/tsmatz/azureml-tutorial-tensorflow-v1/blob/master/notebooks/exercise04_train_remote.ipynb)\"), here I use the built-in curated environment.    \n",
    "Azure Machine Learning provides pre-configured (built-in) image, called **curated environments**, for a variety of purposes. (See [here](https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments) for the list of curated environments.)<br>\n",
    "Here we run distributed training by Horovod and TensorFlow 1.x using a curated environment ```AzureML-TensorFlow-1.13-CPU```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "tf_env = Environment.get(workspace=ws, name='AzureML-TensorFlow-1.13-CPU')\n",
    "src = ScriptRunConfig(\n",
    "    source_directory='./script',\n",
    "    script='train_horovod.py',\n",
    "    arguments=['--data_folder', dataset.as_mount()],\n",
    "    compute_target=compute_target,\n",
    "    environment=tf_env,\n",
    "    distributed_job_config=MpiConfiguration(node_count=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] This ```AzureML-TensorFlow-1.13-CPU``` includes Horovod 0.16.1.    \n",
    "When you want to see the packages included in this curated environment, please run as follows and see the saved configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_env.save_to_directory(path='AzureML-TensorFlow-1.13-CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Run script and wait for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: tf_distribued_1626057509_1c54bf47\n",
      "Web View: https://ml.azure.com/runs/tf_distribued_1626057509_1c54bf47?wsid=/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourcegroups/TEST20210712/workspaces/ws01&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2021-07-12T02:38:46Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=88426 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-07-12T02:38:46Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/mounts/workspaceblobstore\n",
      "2021-07-12T02:38:46Z The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-12T02:38:46Z Starting output-watcher...\n",
      "2021-07-12T02:38:46Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-07-12T02:38:46Z Executing 'Copy ACR Details file' on 10.0.0.10\n",
      "2021-07-12T02:38:46Z Executing 'Copy ACR Details file' on 10.0.0.8\n",
      "2021-07-12T02:38:46Z Executing 'Copy ACR Details file' on 10.0.0.7\n",
      "2021-07-12T02:38:46Z Copy ACR Details file succeeded on 10.0.0.7. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "2021-07-12T02:38:47Z Copy ACR Details file succeeded on 10.0.0.8. Output: \n",
      ">>>   \n",
      "2021-07-12T02:38:47Z Copy ACR Details file succeeded on 10.0.0.10. Output: \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_fe4afc798de401edfb76dc27a38b1703\n",
      "Digest: sha256:5224cd9c4e07c9304c90193ab084da3cf8643e81065eef4d81c2c4029c58248c\n",
      "Status: Image is up to date for viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
      "viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
      "2021-07-12T02:38:48Z The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-12T02:38:48Z Check if container tf_distribued_1626057509_1c54bf47_DataSidecar already exist exited with 0, \n",
      "\n",
      "90ea2dbe360b4682e6beb2b6e7f979cad99589554e9632021c0ac353ce8e6d04\n",
      "2021-07-12T02:38:48Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-07-12T02:38:48Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-708b41783638fdca6e4c2b9bc1bb1473-f8a383ea106c1c16-01 -sshRequired=false] \n",
      "2021/07/12 02:38:48 Starting App Insight Logger for task:  containerSetup\n",
      "2021/07/12 02:38:48 Version: 3.0.01632.0003 Branch: .SourceBranch Commit: 4b96fb0\n",
      "2021/07/12 02:38:48 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/07/12 02:38:48 Starting infiniband setup\n",
      "2021/07/12 02:38:48 Python Version found is Python 3.7.9\n",
      "\n",
      "2021/07/12 02:38:48 Returning Python Version as 3.7\n",
      "2021-07-12T02:38:48Z VMSize: standard_d2_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/07/12 02:38:48 VMSize: standard_d2_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/07/12 02:38:48 VMSize: standard_d2_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021-07-12T02:38:48Z Not setting up Infiniband in Container\n",
      "2021/07/12 02:38:48 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/07/12 02:38:48 Not setting up Infiniband in Container\n",
      "2021/07/12 02:38:48 Not setting up Infiniband in Container\n",
      "2021/07/12 02:38:48 Python Version found is Python 3.7.9\n",
      "\n",
      "2021/07/12 02:38:48 Returning Python Version as 3.7\n",
      "2021/07/12 02:38:48 sshd inside container not required for job, skipping setup.\n",
      "2021/07/12 02:38:49 All App Insights Logs was sent successfully or the close timeout of 20 was reached\n",
      "2021/07/12 02:38:49 App Insight Client has already been closed\n",
      "2021/07/12 02:38:49 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-07-12T02:38:49Z Starting docker container succeeded.\n",
      "2021-07-12T02:38:49Z The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log_0.txt\n",
      "==========================================\n",
      "\n",
      "bash: /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "[2021-07-12T02:39:10.405545] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['train_horovod.py', '--data_folder', 'DatasetConsumptionConfig:input__d39faecc'])\n",
      "This is an MPI job. Rank:0\n",
      "Script type = None\n",
      "[2021-07-12T02:39:11.251124] Entering Run History Context Manager.\n",
      "[2021-07-12T02:39:11.994600] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/wd/azureml/tf_distribued_1626057509_1c54bf47\n",
      "[2021-07-12T02:39:11.994657] Preparing to call script [train_horovod.py] with arguments:['--data_folder', '$input__d39faecc']\n",
      "[2021-07-12T02:39:11.994699] After variable expansion, calling script [train_horovod.py] with arguments:['--data_folder', '/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/wd/input__d39faecc_f3adcd26-eecb-4121-8cb8-b4ca49728bee']\n",
      "\n",
      "/azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From train_horovod.py:26: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/training/input.py:113: RefVariable.count_up_to (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:2132: count_up_to (from tensorflow.python.ops.state_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From train_horovod.py:27: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From train_horovod.py:41: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:91: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "2021-07-12 02:39:13.764333: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-07-12 02:39:13.771430: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394450000 Hz\n",
      "2021-07-12 02:39:13.771616: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4994de0 executing computations on platform Host. Devices:\n",
      "2021-07-12 02:39:13.771647: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:809: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "2021-07-12 02:39:14.867535: W tensorflow/core/framework/allocator.cc:124] Allocation of 67108864 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_33fbf7ecf15f8412d91caaf33900f593/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "current working directory is  /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/wd/azureml/tf_distribued_1626057509_1c54bf47\n",
      "model is saved  b'./outputs/1626057567'\n",
      "\n",
      "\n",
      "[2021-07-12T02:39:27.789152] The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
      "1 items cleaning up...\n",
      "Cleanup took 0.06380271911621094 seconds\n",
      "[2021-07-12T02:39:27.993863] Finished context manager injector.\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2021-07-12T02:39:30.428088] Entering job release\n",
      "[2021-07-12T02:39:31.380676] Starting job release\n",
      "[2021-07-12T02:39:31.381539] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 280\n",
      "[2021-07-12T02:39:31.383937] job release stage : upload_datastore starting...\n",
      "[2021-07-12T02:39:31.395853] Entering context manager injector.\n",
      "[2021-07-12T02:39:31.399347] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-07-12T02:39:31.399503] job release stage : execute_job_release starting...\n",
      "[2021-07-12T02:39:31.400720] job release stage : upload_datastore completed...\n",
      "[2021-07-12T02:39:31.401071] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-07-12T02:39:31.401866] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-07-12T02:39:31.484503] job release stage : send_run_telemetry starting...\n",
      "[2021-07-12T02:39:31.498661] get vm size and vm region successfully.\n",
      "[2021-07-12T02:39:31.506784] get compute meta data successfully.\n",
      "[2021-07-12T02:39:31.534986] job release stage : execute_job_release completed...\n",
      "[2021-07-12T02:39:31.777424] post artifact meta request successfully.\n",
      "[2021-07-12T02:39:31.823297] upload compute record artifact successfully.\n",
      "[2021-07-12T02:39:31.823382] job release stage : send_run_telemetry completed...\n",
      "[2021-07-12T02:39:31.823822] Running in AzureML-Sidecar, starting to exit user context managers...\n",
      "[2021-07-12T02:39:31.823920] Running Sidecar release cmd...\n",
      "[2021-07-12T02:39:31.852244] INFO azureml.sidecar.sidecar: Received task: exit_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/wd/azureml/tf_distribued_1626057509_1c54bf47\n",
      "Enter __exit__ of DatasetContextManager\n",
      "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/wd/input__d39faecc_f3adcd26-eecb-4121-8cb8-b4ca49728bee.\n",
      "fuse: failed to unmount /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/wd/input__d39faecc_f3adcd26-eecb-4121-8cb8-b4ca49728bee: Invalid argument\n",
      "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_distribued_1626057509_1c54bf47/wd/input__d39faecc_f3adcd26-eecb-4121-8cb8-b4ca49728bee.\n",
      "Exit __exit__ of DatasetContextManager\n",
      "[2021-07-12T02:39:31.903534] Removing absolute paths from host...\n",
      "[2021-07-12T02:39:31.903758] INFO azureml.sidecar.task.exit_contexts: Exited Context Managers\n",
      "[2021-07-12T02:39:32.237161] Ran Sidecar release cmd.\n",
      "[2021-07-12T02:39:32.237240] Job release is complete\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: tf_distribued_1626057509_1c54bf47\n",
      "Web View: https://ml.azure.com/runs/tf_distribued_1626057509_1c54bf47?wsid=/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourcegroups/TEST20210712/workspaces/ws01&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'tf_distribued_1626057509_1c54bf47',\n",
       " 'target': 'mycluster01',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2021-07-12T02:38:43.83731Z',\n",
       " 'endTimeUtc': '2021-07-12T02:39:44.94448Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': 'd4d953d9-a970-4bfe-93cd-4aecf9df51e7',\n",
       "  'azureml.git.repository_uri': 'https://github.com/tsmatz/azureml-tutorial-tensorflow-v1.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/tsmatz/azureml-tutorial-tensorflow-v1.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': 'b5e83f1c910ce7bde6489279614a6dace708dfed',\n",
       "  'mlflow.source.git.commit': 'b5e83f1c910ce7bde6489279614a6dace708dfed',\n",
       "  'azureml.git.dirty': 'True',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json',\n",
       "  'azureml.RuntimeType': ''},\n",
       " 'inputDatasets': [{'dataset': {'id': 'f3adcd26-eecb-4121-8cb8-b4ca49728bee'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'input__d39faecc', 'mechanism': 'Mount'}}],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': 'train_horovod.py',\n",
       "  'command': '',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data_folder', 'DatasetConsumptionConfig:input__d39faecc'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'Mpi',\n",
       "  'target': 'mycluster01',\n",
       "  'dataReferences': {},\n",
       "  'data': {'input__d39faecc': {'dataLocation': {'dataset': {'id': 'f3adcd26-eecb-4121-8cb8-b4ca49728bee',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None,\n",
       "     'uri': None},\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'input__d39faecc',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'datacaches': [],\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': 2592000,\n",
       "  'nodeCount': 3,\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'identity': None,\n",
       "  'environment': {'name': 'AzureML-TensorFlow-1.13-CPU',\n",
       "   'version': '45',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-core==1.25.0',\n",
       "        'azureml-defaults==1.25.0',\n",
       "        'azureml-telemetry==1.25.0',\n",
       "        'azureml-train-restclients-hyperdrive==1.25.0',\n",
       "        'azureml-train-core==1.25.0',\n",
       "        'tensorflow==1.13.1',\n",
       "        'horovod==0.16.1']}],\n",
       "     'name': 'azureml_33fbf7ecf15f8412d91caaf33900f593'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210301.v1',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': False,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': None},\n",
       "  'aiSuperComputer': {'instanceType': None,\n",
       "   'imageVersion': None,\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None,\n",
       "   'interactive': False,\n",
       "   'scalePolicy': None,\n",
       "   'virtualClusterArmId': None,\n",
       "   'tensorboardLogDirectory': None,\n",
       "   'sshPublicKey': None,\n",
       "   'enableAzmlInt': True,\n",
       "   'priority': None,\n",
       "   'slaTier': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': False,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []},\n",
       "  'environmentVariables': {},\n",
       "  'applicationEndpoints': {},\n",
       "  'parameters': []},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/55_azureml-execution-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt?sv=2019-02-02&sr=b&sig=PZg6NU8F%2FS5d%2Fio1NQRqF0%2FtC15D5DhZk3Dx8pVzzJ0%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/55_azureml-execution-tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/55_azureml-execution-tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d.txt?sv=2019-02-02&sr=b&sig=mPjaqOiqcHUwAgIhgYXPh47JFVi%2BzEeVZoF6hAWS2Tw%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/55_azureml-execution-tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/55_azureml-execution-tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d.txt?sv=2019-02-02&sr=b&sig=n5zwQnqzR1mtfiCXOkqZ1l%2FqLv9uqfcaNAsUCvhmDZk%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/65_job_prep-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt?sv=2019-02-02&sr=b&sig=9ft7Ztcx9Q9AtIXrbjujji5%2BUIQWjivooVfaR9fHE24%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/65_job_prep-tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d.txt?sv=2019-02-02&sr=b&sig=CcFtfq%2F0Y1uXom0bliJSQbp8IVkwve0splbqIAo%2FThQ%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/65_job_prep-tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d.txt?sv=2019-02-02&sr=b&sig=C9ppau2tpiStFodzn1V7%2FnkIAnYuWi2QoIcbafxzGBI%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/70_driver_log_0.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/70_driver_log_0.txt?sv=2019-02-02&sr=b&sig=uexgFwsZKTscl5fw%2B36PVm3GIT28Q9U6hN6MUbjLk9c%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/70_driver_log_1.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/70_driver_log_1.txt?sv=2019-02-02&sr=b&sig=K9OBxCvUyr0Z0XNsbj3PsvGie1xcKI39NfU5BbzQn14%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/70_driver_log_2.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/70_driver_log_2.txt?sv=2019-02-02&sr=b&sig=A%2B%2BRLx5kr36yQxh9Ug1tOz%2B%2Fd2yLJgUgTrWu9FYsQZw%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/70_mpi_log.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/70_mpi_log.txt?sv=2019-02-02&sr=b&sig=zpd5P5N4sKaS8KwwhxDMANxigjkA4fZx9DqnR8t8cMA%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/75_job_post-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt?sv=2019-02-02&sr=b&sig=8vwc1nFi%2BMaF9EGtldvhojlGYNLY7bpCgzdVpr98uVg%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/75_job_post-tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d.txt?sv=2019-02-02&sr=b&sig=%2BOEyARfs9Pq%2BZd95h7pIvZhzXMWkuZzkzA%2BiZzqMLDo%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d.txt': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/75_job_post-tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d.txt?sv=2019-02-02&sr=b&sig=TVw9zIFiDophcbgC4QpVhdyc%2FBrl%2F7nLw5JgHUmhahM%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=8kEo8x2HZ4gtKWZRgewCUsemJItF2MnAKwqyt%2FVuQmE%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=Ak0pFT7BiFkAW9J%2F7dGemZHhSIbfWLKshFi7Si21ZBY%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=yV6DJ0FR9vxJVKrbmn2Ol3QY6SUezaCfcnN64C9ESFs%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=6xovOwltIh3XsI8lAAtfWMZzLL0tKGJFpUxDBGL%2FocM%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=OSeUn9ZB5Iu3gfyG8LsvKPz9Sc0sCbnfjqpNHFe0Lo8%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d/all.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/logs/azureml/sidecar/tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d/all.log?sv=2019-02-02&sr=b&sig=vyfmwpHpMuHTGDbi8dNbrALGN8SA61FYpSe8EFO5%2FHQ%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d/task.exit_contexts.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/logs/azureml/sidecar/tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=nEis95PAP5lSuisMCZeIQylKiB5NSxyTpbDYs49WzUk%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d/all.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/logs/azureml/sidecar/tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d/all.log?sv=2019-02-02&sr=b&sig=BSqyo9gDL0yefEDuhHs%2FrQsYgKD8Exv7hkMOT06sxk4%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d/task.exit_contexts.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/logs/azureml/sidecar/tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=AWJptmW%2F%2FFxYmm4g2OjcxgwTtVufnzOUCSbx5rwkwDQ%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d/all.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/logs/azureml/sidecar/tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d/all.log?sv=2019-02-02&sr=b&sig=Nt5UCOFFQ35IYVwtHgV4%2FhLkjEva4Lx1NaDwBvTzWSc%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d/task.exit_contexts.log': 'https://ws010492426588.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_distribued_1626057509_1c54bf47/logs/azureml/sidecar/tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=OAMjhNwB5ZhzF4TrtiO1U9OFjvnsE1wcKpD6OPAnGh8%3D&st=2021-07-12T02%3A29%3A45Z&se=2021-07-12T10%3A39%3A45Z&sp=r'},\n",
       " 'submittedBy': 'Tsuyoshi Matsuzaki'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(workspace=ws, name='tf_distribued')\n",
    "run = exp.submit(config=src)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/55_azureml-execution-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt',\n",
       " 'azureml-logs/55_azureml-execution-tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d.txt',\n",
       " 'azureml-logs/55_azureml-execution-tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d.txt',\n",
       " 'azureml-logs/70_driver_log_0.txt',\n",
       " 'azureml-logs/70_driver_log_1.txt',\n",
       " 'azureml-logs/70_driver_log_2.txt',\n",
       " 'azureml-logs/70_mpi_log.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d.txt',\n",
       " 'azureml-logs/process_info.json',\n",
       " 'azureml-logs/process_status.json',\n",
       " 'logs/azureml/dataprep/backgroundProcess.log',\n",
       " 'logs/azureml/dataprep/backgroundProcess_Telemetry.log',\n",
       " 'logs/azureml/job_release_azureml.log',\n",
       " 'logs/azureml/sidecar/tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d/all.log',\n",
       " 'logs/azureml/sidecar/tvmps_50ef9d914602729f6f3634241e9517a271ba8f7237cb370b0e6ac6359bf55006_d/task.exit_contexts.log',\n",
       " 'logs/azureml/sidecar/tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d/all.log',\n",
       " 'logs/azureml/sidecar/tvmps_5fe2606f0ee22c929efd7b00005abd84f5ab562b31a700602884c04423a4eadd_d/task.exit_contexts.log',\n",
       " 'logs/azureml/sidecar/tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d/all.log',\n",
       " 'logs/azureml/sidecar/tvmps_6e40f4ce95034c8811263bcf3983d6bddec7e5caddd8d2a2fd424c961c178f84_d/task.exit_contexts.log',\n",
       " 'logs/checkpoint',\n",
       " 'logs/eval/events.out.tfevents.1626057567.e8641153a3874a29b8dde560ee198ed3000003',\n",
       " 'logs/events.out.tfevents.1626057553.e8641153a3874a29b8dde560ee198ed3000003',\n",
       " 'logs/graph.pbtxt',\n",
       " 'logs/model.ckpt-0.data-00000-of-00001',\n",
       " 'logs/model.ckpt-0.index',\n",
       " 'logs/model.ckpt-0.meta',\n",
       " 'logs/model.ckpt-400.data-00000-of-00001',\n",
       " 'logs/model.ckpt-400.index',\n",
       " 'logs/model.ckpt-400.meta',\n",
       " 'outputs/1626057567/saved_model.pb',\n",
       " 'outputs/1626057567/variables/variables.data-00000-of-00001',\n",
       " 'outputs/1626057567/variables/variables.index']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please change ```1626057567``` to meet previous results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(\n",
    "    name='outputs/1626057567/saved_model.pb',\n",
    "    output_file_path='distributed_model/saved_model.pb')\n",
    "run.download_file(\n",
    "    name='outputs/1626057567/variables/variables.data-00000-of-00001',\n",
    "    output_file_path='distributed_model/variables/variables.data-00000-of-00001')\n",
    "run.download_file(\n",
    "    name='outputs/1626057567/variables/variables.index',\n",
    "    output_file_path='distributed_model/variables/variables.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from ./distributed_model/variables/variables\n",
      "Predicted:  [7, 2, 1]\n",
      "Actual   :  [7, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Read data by tensor\n",
    "tfdata = tf.data.TFRecordDataset('./data/test.tfrecords')\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(tfdata)\n",
    "data_org = iterator.get_next()\n",
    "data_exam = tf.parse_single_example(\n",
    "    data_org,\n",
    "    features={\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "        'label': tf.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "data_image = tf.decode_raw(data_exam['image_raw'], tf.uint8)\n",
    "data_image.set_shape([784])\n",
    "data_image = tf.cast(data_image, tf.float32) * (1. / 255)\n",
    "data_label = tf.cast(data_exam['label'], tf.int32)\n",
    "\n",
    "# Run tensor and generate data\n",
    "with tf.Session() as sess:\n",
    "    image_arr = []\n",
    "    label_arr = []\n",
    "    for i in range(3):\n",
    "        image, label = sess.run([data_image, data_label])\n",
    "        image_arr.append(image)\n",
    "        label_arr.append(label)\n",
    "\n",
    "# Predict\n",
    "pred_fn = tf.contrib.predictor.from_saved_model('./distributed_model')\n",
    "pred = pred_fn({'inputs': image_arr})\n",
    "\n",
    "print('Predicted: ', pred['classes'].tolist())\n",
    "print('Actual   : ', label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 : Remove AML compute\n",
    "\n",
    "**You don't need to remove your AML compute** for saving money, because the nodes will be automatically terminated, when it's inactive.    \n",
    "But if you want to clean up, please run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete cluster (nbodes) and remove from AML workspace\n",
    "mycompute = AmlCompute(workspace=ws, name='mycluster01')\n",
    "mycompute.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
