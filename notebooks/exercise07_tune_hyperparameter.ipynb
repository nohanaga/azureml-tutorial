{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise07 : Hyperparameter Tuning\n",
    "\n",
    "AML provides framework-independent hyperparameter tuning capability.    \n",
    "This capability monitors accuracy in AML logs.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your training code\n",
    "\n",
    "First, you must save your training code.    \n",
    "Here we should use the source code in \"[Exercise06 : Experimentation Logs and Outputs](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/blob/master/notebooks/exercise06_experimentation.ipynb)\", which sends logs periodically into AML run history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ```scirpt``` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save source code as ```./script/train_expriment.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script/train_experiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/train_experiment.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from azureml.core.run import Run\n",
    "\n",
    "# Get run when running in remote\n",
    "if 'run' not in locals():\n",
    "    run = Run.get_context()\n",
    "\n",
    "FLAGS = None\n",
    "batch_size = 100\n",
    "\n",
    "#\n",
    "# define functions for Estimator\n",
    "#\n",
    "\n",
    "def _my_input_fn(filepath, num_epochs):\n",
    "    # image - 784 (=28 x 28) elements of grey-scaled integer value [0, 1]\n",
    "    # label - digit (0, 1, ..., 9)\n",
    "    data_queue = tf.train.string_input_producer(\n",
    "        [filepath],\n",
    "        num_epochs = num_epochs) # data is repeated and it raises OutOfRange when data is over\n",
    "    data_reader = tf.TFRecordReader()\n",
    "    _, serialized_exam = data_reader.read(data_queue)\n",
    "    data_exam = tf.parse_single_example(\n",
    "        serialized_exam,\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    data_image = tf.decode_raw(data_exam['image_raw'], tf.uint8)\n",
    "    data_image.set_shape([784])\n",
    "    data_image = tf.cast(data_image, tf.float32) * (1. / 255)\n",
    "    data_label = tf.cast(data_exam['label'], tf.int32)\n",
    "    data_batch_image, data_batch_label = tf.train.batch(\n",
    "        [data_image, data_label],\n",
    "        batch_size=batch_size)\n",
    "    return {'inputs': data_batch_image}, data_batch_label\n",
    "\n",
    "def _get_input_fn(filepath, num_epochs):\n",
    "    return lambda: _my_input_fn(filepath, num_epochs)\n",
    "\n",
    "def _my_model_fn(features, labels, mode):\n",
    "    # with tf.device(...): # You can set device if using GPUs\n",
    "\n",
    "    # define network and inference\n",
    "    # (simple 2 fully connected hidden layer : 784->128->64->10)\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [784, FLAGS.first_layer],\n",
    "                stddev=1.0 / math.sqrt(float(784))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.first_layer]),\n",
    "            name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(features['inputs'], weights) + biases)\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.first_layer, FLAGS.second_layer],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.first_layer))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.second_layer]),\n",
    "            name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.second_layer, 10],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.second_layer))),\n",
    "        name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([10]),\n",
    "            name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    " \n",
    "    # compute evaluation matrix\n",
    "    predicted_indices = tf.argmax(input=logits, axis=1)\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        label_indices = tf.cast(labels, tf.int32)\n",
    "        accuracy = tf.metrics.accuracy(label_indices, predicted_indices)\n",
    "        tf.summary.scalar('accuracy', accuracy[1]) # output to TensorBoard \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=labels,\n",
    "            logits=logits)\n",
    " \n",
    "    # define operations\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        #global_step = tf.train.create_global_step()\n",
    "        #global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        global_step = tf.train.get_or_create_global_step()        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=FLAGS.learning_rate)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=global_step)\n",
    "        # Ask for accuracy and loss in each steps\n",
    "        class _CustomLoggingHook(tf.train.SessionRunHook):\n",
    "            def begin(self):\n",
    "                self.training_accuracy = []\n",
    "                self.training_loss = []\n",
    "            def before_run(self, run_context):\n",
    "                return tf.train.SessionRunArgs([accuracy[1], loss, global_step])\n",
    "            def after_run(self, run_context, run_values):\n",
    "                result_accuracy, result_loss, result_step = run_values.results\n",
    "                if result_step % 10 == 0 :\n",
    "                    self.training_accuracy.append(result_accuracy)\n",
    "                    self.training_loss.append(result_loss)\n",
    "                if result_step % 100 == 0 : # save logs in each 100 steps\n",
    "                    run.log_list('training_accuracy', self.training_accuracy)\n",
    "                    run.log_list('training_loss', self.training_loss)\n",
    "                    self.training_accuracy = []\n",
    "                    self.training_loss = []\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            training_chief_hooks=[_CustomLoggingHook()],\n",
    "            loss=loss,\n",
    "            train_op=train_op)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        eval_metric_ops = {\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
    "        predictions = {\n",
    "            'classes': predicted_indices,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "        export_outputs = {\n",
    "            'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            predictions=predictions,\n",
    "            export_outputs=export_outputs)\n",
    "\n",
    "def _my_serving_input_fn():\n",
    "    inputs = {'inputs': tf.placeholder(tf.float32, [None, 784])}\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
    "\n",
    "#\n",
    "# Main\n",
    "#\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--data_folder',\n",
    "    type=str,\n",
    "    default='./data',\n",
    "    help='Folder path for input data')\n",
    "parser.add_argument(\n",
    "    '--chkpoint_folder',\n",
    "    type=str,\n",
    "    default='./logs',  # AML experiments logs folder\n",
    "    help='Folder path for checkpoint files')\n",
    "parser.add_argument(\n",
    "    '--model_folder',\n",
    "    type=str,\n",
    "    default='./outputs',  # AML experiments outputs folder\n",
    "    help='Folder path for model output')\n",
    "parser.add_argument(\n",
    "    '--learning_rate',\n",
    "    type=float,\n",
    "    default='0.07',\n",
    "    help='Learning Rate')\n",
    "parser.add_argument(\n",
    "    '--first_layer',\n",
    "    type=int,\n",
    "    default='128',\n",
    "    help='Neuron number for the first hidden layer')\n",
    "parser.add_argument(\n",
    "    '--second_layer',\n",
    "    type=int,\n",
    "    default='64',\n",
    "    help='Neuron number for the second hidden layer')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# clean checkpoint and model folder if exists\n",
    "if os.path.exists(FLAGS.chkpoint_folder) :\n",
    "    for file_name in os.listdir(FLAGS.chkpoint_folder):\n",
    "        file_path = os.path.join(FLAGS.chkpoint_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "if os.path.exists(FLAGS.model_folder) :\n",
    "    for file_name in os.listdir(FLAGS.model_folder):\n",
    "        file_path = os.path.join(FLAGS.model_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "\n",
    "# read TF_CONFIG\n",
    "run_config = tf.estimator.RunConfig()\n",
    "\n",
    "# create Estimator\n",
    "mnist_fullyconnected_classifier = tf.estimator.Estimator(\n",
    "    model_fn=_my_model_fn,\n",
    "    model_dir=FLAGS.chkpoint_folder,\n",
    "    config=run_config)\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'train.tfrecords'), 2),\n",
    "    max_steps=60000 * 2 / batch_size)\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'test.tfrecords'), 1),\n",
    "    steps=10000 * 1 / batch_size,\n",
    "    start_delay_secs=0)\n",
    "\n",
    "# run !\n",
    "eval_res = tf.estimator.train_and_evaluate(\n",
    "    mnist_fullyconnected_classifier,\n",
    "    train_spec,\n",
    "    eval_spec\n",
    ")\n",
    "\n",
    "# save model and variables\n",
    "model_dir = mnist_fullyconnected_classifier.export_savedmodel(\n",
    "    export_dir_base = FLAGS.model_folder,\n",
    "    serving_input_receiver_fn = _my_serving_input_fn)\n",
    "print('current working directory is ', os.getcwd())\n",
    "print('model is saved ', model_dir)\n",
    "\n",
    "# send logs to AML\n",
    "run.log('learning_rate', FLAGS.learning_rate)\n",
    "run.log('1st_layer', FLAGS.first_layer)\n",
    "run.log('2nd_layer', FLAGS.second_layer)\n",
    "run.log('final_accuracy', eval_res[0]['accuracy'])\n",
    "run.log('final_loss', eval_res[0]['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get workspace setting\n",
    "\n",
    "Before starting, you must read your configuration settings. (See \"[Exercise01 : Prepare Config Settings](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/blob/master/notebooks/exercise01_prepare_config.ipynb)\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import azureml.core\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML compute\n",
    "\n",
    "Create AML compute pool for computing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new.\n",
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name='hypertest01')\n",
    "    print('found existing:', compute_target.name)\n",
    "except ComputeTargetException:\n",
    "    print('creating new.')\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='Standard_D2_v2',\n",
    "        min_nodes=0,\n",
    "        max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, 'hypertest01', compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-05-13T02:21:20.180000+00:00', 'errors': None, 'creationTime': '2019-05-13T02:20:48.612965+00:00', 'modifiedTime': '2019-05-13T02:21:34.624887+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
     ]
    }
   ],
   "source": [
    "# get a status for the current cluster.\n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datastore\n",
    "\n",
    "You can mount your datastore (See \"[Exercise02 : Prepare Datastore](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/blob/master/notebooks/exercise02_prepare_datastore.ipynb)\") into your Batch AI compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "\n",
    "# get common datastore (See \"Exercise 02 : Prepare Datastore\")\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# generate data reference configuration\n",
    "dr_conf = DataReferenceConfiguration(\n",
    "    datastore_name=ds.name,\n",
    "    path_on_datastore='tfdata',\n",
    "    mode='mount') # set 'download' if you copy all files instead of mounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hyperparameter Sampling\n",
    "\n",
    "Set how to explorer for script (```train_experiment.py```) parameters.    \n",
    "You can choose from ```GridParameterSampling```, ```RandomParameterSampling```, and ```BayesianParameterSampling```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import *\n",
    "\n",
    "param_sampling = RandomParameterSampling(\n",
    "    {\n",
    "        '--learning_rate': choice(0.01, 0.05, 0.9),\n",
    "        '--first_layer': choice(100, 125, 150),\n",
    "        '--second_layer': choice(30, 60, 90)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate script run config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, Run, ScriptRunConfig\n",
    "from azureml.core.runconfig import RunConfiguration, DockerConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# generate run config\n",
    "run_config = RunConfiguration(\n",
    "    framework=\"python\",\n",
    "    conda_dependencies=CondaDependencies.create(conda_packages=['tensorflow==1.15']))\n",
    "run_config.target = compute_target.name\n",
    "run_config.data_references = {ds.name: dr_conf}\n",
    "run_config.docker = DockerConfiguration(use_docker=True)\n",
    "\n",
    "# generate script run config\n",
    "src = ScriptRunConfig(\n",
    "    source_directory='./script',\n",
    "    script='train_experiment.py',\n",
    "    run_config=run_config,\n",
    "    arguments=['--data_folder', str(ds.as_mount())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate HyperDrive config\n",
    "\n",
    "Generate run config with an early termnination policy (```BanditPolicy```). With this policy, the training will terminate if the primary metric falls outside of the top 10% range (checking every 2 iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early termnination :\n",
    "# primary metric falls outside of the top 10% (0.1) range by checking every 2 iterations\n",
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "\n",
    "# generate run config\n",
    "hd_config = HyperDriveConfig(\n",
    "    run_config=src,\n",
    "    hyperparameter_sampling=param_sampling,\n",
    "    primary_metric_name='training_accuracy',\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "    policy=policy,\n",
    "    max_total_runs=20,\n",
    "    max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run script and wait for completion\n",
    "\n",
    "This will start training with 4 parallel nodes. (You can scale as you like.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: hyperdrive_test_1557714325126\n",
      "\n",
      "Streaming azureml-logs/hyperdrive.txt\n",
      "=====================================\n",
      "\n",
      "\"<START>[2019-05-13T02:25:25.616384][API][INFO]Experiment created<END>\\n\"\"<START>[2019-05-13T02:25:26.166956][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space<END>\\n\"\"<START>[2019-05-13T02:25:26.428769][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.<END>\\n\"<START>[2019-05-13T02:25:28.2107641Z][SCHEDULER][INFO]The execution environment is being prepared. Please be patient as it can take a few minutes.<END>\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: hyperdrive_test_1557714325126\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'hyperdrive_test_1557714325126',\n",
       " 'target': 'hypertest01',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2019-05-13T02:25:25.439516Z',\n",
       " 'endTimeUtc': '2019-05-13T02:39:25.000Z',\n",
       " 'properties': {'primary_metric_config': '{\"name\": \"training_accuracy\", \"goal\": \"maximize\"}',\n",
       "  'runTemplate': 'HyperDrive',\n",
       "  'azureml.runsource': 'hyperdrive',\n",
       "  'ContentSnapshotId': 'a3c79a84-936e-4b13-b02e-aa3b8857bd92'},\n",
       " 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://ws014089428540.blob.core.windows.net/azureml/ExperimentRun/dcid.hyperdrive_test_1557714325126/azureml-logs/hyperdrive.txt?sv=2018-03-28&sr=b&sig=atOUYCOXqV7qUU5iHBf3eOp41zZT73Ey5j7aSL4xuH8%3D&st=2019-05-13T02%3A29%3A27Z&se=2019-05-13T10%3A39%3A27Z&sp=r'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(workspace=ws, name='hyperdrive_test')\n",
    "run = experiment.submit(config=hd_config)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view logs using [Azure Portal](https://portal.azure.com/), but you can also view using AML run history widget in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d54610e9c7d48feadeadb8e0a3dcd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run_instance=run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explorer metrics with your python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyperdrive_test_1557714325126_1': {'learning_rate': [0.05], '1st_layer': [150], '2nd_layer': [60], 'final_accuracy': [0.9279000163078308], 'final_loss': [0.24563941359519958]}, 'hyperdrive_test_1557714325126_3': {'learning_rate': [0.01], '1st_layer': [150], '2nd_layer': [30], 'final_accuracy': [0.8830000162124634], 'final_loss': [0.46186450123786926]}, 'hyperdrive_test_1557714325126_0': {'learning_rate': [0.05], '1st_layer': [125], '2nd_layer': [60], 'final_accuracy': [0.9223999977111816], 'final_loss': [0.26270031929016113]}, 'hyperdrive_test_1557714325126_2': {'learning_rate': [0.05], '1st_layer': [100], '2nd_layer': [60], 'final_accuracy': [0.9229999780654907], 'final_loss': [0.25971081852912903]}, 'hyperdrive_test_1557714325126_4': {'learning_rate': [0.01], '1st_layer': [100], '2nd_layer': [60], 'final_accuracy': [0.8781999945640564], 'final_loss': [0.4436098039150238]}, 'hyperdrive_test_1557714325126_5': {'learning_rate': [0.05], '1st_layer': [150], '2nd_layer': [60], 'final_accuracy': [0.9229999780654907], 'final_loss': [0.2620431184768677]}, 'hyperdrive_test_1557714325126_7': {'learning_rate': [0.01], '1st_layer': [100], '2nd_layer': [60], 'final_accuracy': [0.8716999888420105], 'final_loss': [0.46906578540802]}, 'hyperdrive_test_1557714325126_6': {'learning_rate': [0.05], '1st_layer': [150], '2nd_layer': [30], 'final_accuracy': [0.9222000241279602], 'final_loss': [0.2593305706977844]}, 'hyperdrive_test_1557714325126_8': {'learning_rate': [0.05], '1st_layer': [150], '2nd_layer': [60], 'final_accuracy': [0.923799991607666], 'final_loss': [0.25586873292922974]}, 'hyperdrive_test_1557714325126_11': {'learning_rate': [0.05], '1st_layer': [150], '2nd_layer': [60], 'final_accuracy': [0.9244999885559082], 'final_loss': [0.2613087594509125]}, 'hyperdrive_test_1557714325126_10': {'learning_rate': [0.05], '1st_layer': [100], '2nd_layer': [90], 'final_accuracy': [0.9232000112533569], 'final_loss': [0.261786550283432]}, 'hyperdrive_test_1557714325126_13': {'learning_rate': [0.01], '1st_layer': [100], '2nd_layer': [60], 'final_accuracy': [0.8694000244140625], 'final_loss': [0.4778388738632202]}, 'hyperdrive_test_1557714325126_12': {'learning_rate': [0.05], '1st_layer': [150], '2nd_layer': [60], 'final_accuracy': [0.9258999824523926], 'final_loss': [0.25292903184890747]}, 'hyperdrive_test_1557714325126_14': {'learning_rate': [0.05], '1st_layer': [150], '2nd_layer': [60], 'final_accuracy': [0.9271000027656555], 'final_loss': [0.24681158363819122]}, 'hyperdrive_test_1557714325126_15': {'learning_rate': [0.9], '1st_layer': [150], '2nd_layer': [90], 'final_accuracy': [0.21490000188350677], 'final_loss': [1.961561679840088]}, 'hyperdrive_test_1557714325126_17': {'learning_rate': [0.05], '1st_layer': [125], '2nd_layer': [60], 'final_accuracy': [0.9221000075340271], 'final_loss': [0.2596137523651123]}, 'hyperdrive_test_1557714325126_16': {'learning_rate': [0.01], '1st_layer': [125], '2nd_layer': [90], 'final_accuracy': [0.8827999830245972], 'final_loss': [0.4486849308013916]}, 'hyperdrive_test_1557714325126_18': {'learning_rate': [0.05], '1st_layer': [125], '2nd_layer': [60], 'final_accuracy': [0.9241999983787537], 'final_loss': [0.2561037540435791]}, 'hyperdrive_test_1557714325126_19': {'learning_rate': [0.9], '1st_layer': [150], '2nd_layer': [90], 'final_accuracy': [0.9516000151634216], 'final_loss': [0.16374324262142181]}, 'hyperdrive_test_1557714325126_9': {'learning_rate': [0.05], '1st_layer': [150], '2nd_layer': [90], 'final_accuracy': [0.9247999787330627], 'final_loss': [0.2595297396183014]}}\n"
     ]
    }
   ],
   "source": [
    "allmetrics = run.get_metrics()\n",
    "print(allmetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove AML compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete cluster (nbodes) and remove from AML workspace\n",
    "mycompute = AmlCompute(workspace=ws, name='hypertest01')\n",
    "mycompute.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
